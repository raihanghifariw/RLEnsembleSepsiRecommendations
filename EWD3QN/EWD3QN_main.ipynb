{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim\n",
    "from WD3QNE_deepQnet import WD3QNE\n",
    "from WD3QNE_evaluate import do_eval, do_test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================== small function===============================\n",
    "from scipy.stats import zscore, rankdata\n",
    "\n",
    "\n",
    "def my_zscore(x):\n",
    "    return zscore(x, ddof=1), np.mean(x, axis=0), np.std(x, axis=0, ddof=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####  generated state  ####\n",
      "####  Generating Actions  ####\n",
      "####  Generate Rewards  ####\n",
      "380394\n",
      "####  Generate trajectory  ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float().to(self.device)\n",
      "d:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(next_state).float().to(self.device)\n",
      "d:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action).long().to(self.device)\n",
      "d:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward = torch.tensor(reward).float().to(self.device)\n",
      "d:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  done = torch.tensor(done).float().to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Average Loss: 2.2045\n",
      "Epoch: 0, Batch: 25, Average Loss: 2.1792\n",
      "Epoch: 0, Batch: 50, Average Loss: 2.1546\n",
      "Epoch: 0, Batch: 75, Average Loss: 2.1481\n",
      "Epoch: 0, Batch: 100, Average Loss: 2.1493\n",
      "Epoch: 0, Batch: 125, Average Loss: 2.1510\n",
      "Epoch: 0, Batch: 150, Average Loss: 2.1504\n",
      "Epoch: 0, Batch: 175, Average Loss: 2.1495\n",
      "Epoch: 0, Batch: 200, Average Loss: 2.1518\n",
      "Epoch 0 - Mean agent Q: 0.15070365369319916, Mean phys Q: 0.1006375327706337\n",
      "New best model saved with mean agent Q: 0.15070365369319916\n",
      "Agent actions: tensor([0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 0, 6, 6, 6, 6, 6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 1, Batch: 0, Average Loss: 2.1715\n",
      "Epoch: 1, Batch: 25, Average Loss: 2.1489\n",
      "Epoch: 1, Batch: 50, Average Loss: 2.1244\n",
      "Epoch: 1, Batch: 75, Average Loss: 2.1165\n",
      "Epoch: 1, Batch: 100, Average Loss: 2.1164\n",
      "Epoch: 1, Batch: 125, Average Loss: 2.1167\n",
      "Epoch: 1, Batch: 150, Average Loss: 2.1152\n",
      "Epoch: 1, Batch: 175, Average Loss: 2.1136\n",
      "Epoch: 1, Batch: 200, Average Loss: 2.1153\n",
      "Epoch 1 - Mean agent Q: 0.1628858745098114, Mean phys Q: 0.10567140579223633\n",
      "New best model saved with mean agent Q: 0.1628858745098114\n",
      "Agent actions: tensor([0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 2, Batch: 0, Average Loss: 2.1393\n",
      "Epoch: 2, Batch: 25, Average Loss: 2.1083\n",
      "Epoch: 2, Batch: 50, Average Loss: 2.0850\n",
      "Epoch: 2, Batch: 75, Average Loss: 2.0768\n",
      "Epoch: 2, Batch: 100, Average Loss: 2.0769\n",
      "Epoch: 2, Batch: 125, Average Loss: 2.0764\n",
      "Epoch: 2, Batch: 150, Average Loss: 2.0745\n",
      "Epoch: 2, Batch: 175, Average Loss: 2.0722\n",
      "Epoch: 2, Batch: 200, Average Loss: 2.0731\n",
      "Epoch 2 - Mean agent Q: 0.24912995100021362, Mean phys Q: 0.19099347293376923\n",
      "New best model saved with mean agent Q: 0.24912995100021362\n",
      "Agent actions: tensor([0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 3, Batch: 0, Average Loss: 2.0940\n",
      "Epoch: 3, Batch: 25, Average Loss: 2.0543\n",
      "Epoch: 3, Batch: 50, Average Loss: 2.0312\n",
      "Epoch: 3, Batch: 75, Average Loss: 2.0226\n",
      "Epoch: 3, Batch: 100, Average Loss: 2.0226\n",
      "Epoch: 3, Batch: 125, Average Loss: 2.0214\n",
      "Epoch: 3, Batch: 150, Average Loss: 2.0190\n",
      "Epoch: 3, Batch: 175, Average Loss: 2.0162\n",
      "Epoch: 3, Batch: 200, Average Loss: 2.0164\n",
      "Epoch 3 - Mean agent Q: 0.3596596121788025, Mean phys Q: 0.30662092566490173\n",
      "New best model saved with mean agent Q: 0.3596596121788025\n",
      "Agent actions: tensor([20,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  6,  6,  6,  6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 4, Batch: 0, Average Loss: 2.0412\n",
      "Epoch: 4, Batch: 25, Average Loss: 1.9919\n",
      "Epoch: 4, Batch: 50, Average Loss: 1.9690\n",
      "Epoch: 4, Batch: 75, Average Loss: 1.9607\n",
      "Epoch: 4, Batch: 100, Average Loss: 1.9610\n",
      "Epoch: 4, Batch: 125, Average Loss: 1.9598\n",
      "Epoch: 4, Batch: 150, Average Loss: 1.9577\n",
      "Epoch: 4, Batch: 175, Average Loss: 1.9549\n",
      "Epoch: 4, Batch: 200, Average Loss: 1.9550\n",
      "Epoch 4 - Mean agent Q: 0.4595915973186493, Mean phys Q: 0.41322949528694153\n",
      "New best model saved with mean agent Q: 0.4595915973186493\n",
      "Agent actions: tensor([20,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  6,  6,  6,  6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 5, Batch: 0, Average Loss: 1.9914\n",
      "Epoch: 5, Batch: 25, Average Loss: 1.9331\n",
      "Epoch: 5, Batch: 50, Average Loss: 1.9103\n",
      "Epoch: 5, Batch: 75, Average Loss: 1.9028\n",
      "Epoch: 5, Batch: 100, Average Loss: 1.9035\n",
      "Epoch: 5, Batch: 125, Average Loss: 1.9029\n",
      "Epoch: 5, Batch: 150, Average Loss: 1.9012\n",
      "Epoch: 5, Batch: 175, Average Loss: 1.8990\n",
      "Epoch: 5, Batch: 200, Average Loss: 1.8992\n",
      "Epoch 5 - Mean agent Q: 0.553945004940033, Mean phys Q: 0.5137614011764526\n",
      "New best model saved with mean agent Q: 0.553945004940033\n",
      "Agent actions: tensor([20,  6,  0,  6,  0,  0,  0,  0,  0,  0,  0,  6,  6,  6,  6,  6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 6, Batch: 0, Average Loss: 1.9495\n",
      "Epoch: 6, Batch: 25, Average Loss: 1.8851\n",
      "Epoch: 6, Batch: 50, Average Loss: 1.8625\n",
      "Epoch: 6, Batch: 75, Average Loss: 1.8558\n",
      "Epoch: 6, Batch: 100, Average Loss: 1.8570\n",
      "Epoch: 6, Batch: 125, Average Loss: 1.8570\n",
      "Epoch: 6, Batch: 150, Average Loss: 1.8560\n",
      "Epoch: 6, Batch: 175, Average Loss: 1.8545\n",
      "Epoch: 6, Batch: 200, Average Loss: 1.8550\n",
      "Epoch 6 - Mean agent Q: 0.6322262287139893, Mean phys Q: 0.5968078970909119\n",
      "New best model saved with mean agent Q: 0.6322262287139893\n",
      "Agent actions: tensor([20,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  6,  6,  6,  6,  6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 7, Batch: 0, Average Loss: 1.9172\n",
      "Epoch: 7, Batch: 25, Average Loss: 1.8496\n",
      "Epoch: 7, Batch: 50, Average Loss: 1.8273\n",
      "Epoch: 7, Batch: 75, Average Loss: 1.8216\n",
      "Epoch: 7, Batch: 100, Average Loss: 1.8231\n",
      "Epoch: 7, Batch: 125, Average Loss: 1.8237\n",
      "Epoch: 7, Batch: 150, Average Loss: 1.8231\n",
      "Epoch: 7, Batch: 175, Average Loss: 1.8223\n",
      "Epoch: 7, Batch: 200, Average Loss: 1.8231\n",
      "Epoch 7 - Mean agent Q: 0.7065472602844238, Mean phys Q: 0.6733102798461914\n",
      "New best model saved with mean agent Q: 0.7065472602844238\n",
      "Agent actions: tensor([8, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 8, Batch: 0, Average Loss: 1.8936\n",
      "Epoch: 8, Batch: 25, Average Loss: 1.8248\n",
      "Epoch: 8, Batch: 50, Average Loss: 1.8030\n",
      "Epoch: 8, Batch: 75, Average Loss: 1.7979\n",
      "Epoch: 8, Batch: 100, Average Loss: 1.7996\n",
      "Epoch: 8, Batch: 125, Average Loss: 1.8008\n",
      "Epoch: 8, Batch: 150, Average Loss: 1.8006\n",
      "Epoch: 8, Batch: 175, Average Loss: 1.8002\n",
      "Epoch: 8, Batch: 200, Average Loss: 1.8013\n",
      "Epoch 8 - Mean agent Q: 0.7855191230773926, Mean phys Q: 0.7578594088554382\n",
      "New best model saved with mean agent Q: 0.7855191230773926\n",
      "Agent actions: tensor([8, 1, 8, 8, 8, 0, 0, 0, 0, 0, 0, 6, 1, 6, 6, 6])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 9, Batch: 0, Average Loss: 1.8766\n",
      "Epoch: 9, Batch: 25, Average Loss: 1.8081\n",
      "Epoch: 9, Batch: 50, Average Loss: 1.7868\n",
      "Epoch: 9, Batch: 75, Average Loss: 1.7823\n",
      "Epoch: 9, Batch: 100, Average Loss: 1.7841\n",
      "Epoch: 9, Batch: 125, Average Loss: 1.7858\n",
      "Epoch: 9, Batch: 150, Average Loss: 1.7859\n",
      "Epoch: 9, Batch: 175, Average Loss: 1.7859\n",
      "Epoch: 9, Batch: 200, Average Loss: 1.7873\n",
      "Epoch 9 - Mean agent Q: 0.8771142363548279, Mean phys Q: 0.8484310507774353\n",
      "New best model saved with mean agent Q: 0.8771142363548279\n",
      "Agent actions: tensor([9, 1, 8, 8, 8, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 10, Batch: 0, Average Loss: 1.8670\n",
      "Epoch: 10, Batch: 25, Average Loss: 1.7986\n",
      "Epoch: 10, Batch: 50, Average Loss: 1.7776\n",
      "Epoch: 10, Batch: 75, Average Loss: 1.7737\n",
      "Epoch: 10, Batch: 100, Average Loss: 1.7755\n",
      "Epoch: 10, Batch: 125, Average Loss: 1.7775\n",
      "Epoch: 10, Batch: 150, Average Loss: 1.7778\n",
      "Epoch: 10, Batch: 175, Average Loss: 1.7781\n",
      "Epoch: 10, Batch: 200, Average Loss: 1.7798\n",
      "Epoch 10 - Mean agent Q: 0.9754410982131958, Mean phys Q: 0.9413320422172546\n",
      "New best model saved with mean agent Q: 0.9754410982131958\n",
      "Agent actions: tensor([9, 1, 8, 9, 9, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 11, Batch: 0, Average Loss: 1.8618\n",
      "Epoch: 11, Batch: 25, Average Loss: 1.7943\n",
      "Epoch: 11, Batch: 50, Average Loss: 1.7736\n",
      "Epoch: 11, Batch: 75, Average Loss: 1.7701\n",
      "Epoch: 11, Batch: 100, Average Loss: 1.7719\n",
      "Epoch: 11, Batch: 125, Average Loss: 1.7742\n",
      "Epoch: 11, Batch: 150, Average Loss: 1.7745\n",
      "Epoch: 11, Batch: 175, Average Loss: 1.7750\n",
      "Epoch: 11, Batch: 200, Average Loss: 1.7769\n",
      "Epoch 11 - Mean agent Q: 1.0802969932556152, Mean phys Q: 1.0368590354919434\n",
      "New best model saved with mean agent Q: 1.0802969932556152\n",
      "Agent actions: tensor([9, 1, 9, 9, 9, 9, 0, 0, 0, 0, 0, 3, 1, 1, 1, 9])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 12, Batch: 0, Average Loss: 1.8614\n",
      "Epoch: 12, Batch: 25, Average Loss: 1.7942\n",
      "Epoch: 12, Batch: 50, Average Loss: 1.7737\n",
      "Epoch: 12, Batch: 75, Average Loss: 1.7708\n",
      "Epoch: 12, Batch: 100, Average Loss: 1.7726\n",
      "Epoch: 12, Batch: 125, Average Loss: 1.7752\n",
      "Epoch: 12, Batch: 150, Average Loss: 1.7755\n",
      "Epoch: 12, Batch: 175, Average Loss: 1.7762\n",
      "Epoch: 12, Batch: 200, Average Loss: 1.7784\n",
      "Epoch 12 - Mean agent Q: 1.1799616813659668, Mean phys Q: 1.127142071723938\n",
      "New best model saved with mean agent Q: 1.1799616813659668\n",
      "Agent actions: tensor([9, 4, 9, 9, 9, 9, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 13, Batch: 0, Average Loss: 1.8658\n",
      "Epoch: 13, Batch: 25, Average Loss: 1.7989\n",
      "Epoch: 13, Batch: 50, Average Loss: 1.7786\n",
      "Epoch: 13, Batch: 75, Average Loss: 1.7763\n",
      "Epoch: 13, Batch: 100, Average Loss: 1.7781\n",
      "Epoch: 13, Batch: 125, Average Loss: 1.7810\n",
      "Epoch: 13, Batch: 150, Average Loss: 1.7812\n",
      "Epoch: 13, Batch: 175, Average Loss: 1.7822\n",
      "Epoch: 13, Batch: 200, Average Loss: 1.7847\n",
      "Epoch 13 - Mean agent Q: 1.2986911535263062, Mean phys Q: 1.2240869998931885\n",
      "New best model saved with mean agent Q: 1.2986911535263062\n",
      "Agent actions: tensor([9, 4, 4, 4, 4, 8, 8, 9, 0, 0, 0, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 14, Batch: 0, Average Loss: 1.8771\n",
      "Epoch: 14, Batch: 25, Average Loss: 1.8094\n",
      "Epoch: 14, Batch: 50, Average Loss: 1.7894\n",
      "Epoch: 14, Batch: 75, Average Loss: 1.7877\n",
      "Epoch: 14, Batch: 100, Average Loss: 1.7894\n",
      "Epoch: 14, Batch: 125, Average Loss: 1.7926\n",
      "Epoch: 14, Batch: 150, Average Loss: 1.7928\n",
      "Epoch: 14, Batch: 175, Average Loss: 1.7942\n",
      "Epoch: 14, Batch: 200, Average Loss: 1.7971\n",
      "Epoch 14 - Mean agent Q: 1.418644666671753, Mean phys Q: 1.323956847190857\n",
      "New best model saved with mean agent Q: 1.418644666671753\n",
      "Agent actions: tensor([9, 4, 4, 4, 4, 4, 8, 8, 8, 0, 0, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 15, Batch: 0, Average Loss: 1.8968\n",
      "Epoch: 15, Batch: 25, Average Loss: 1.8270\n",
      "Epoch: 15, Batch: 50, Average Loss: 1.8073\n",
      "Epoch: 15, Batch: 75, Average Loss: 1.8064\n",
      "Epoch: 15, Batch: 100, Average Loss: 1.8080\n",
      "Epoch: 15, Batch: 125, Average Loss: 1.8116\n",
      "Epoch: 15, Batch: 150, Average Loss: 1.8117\n",
      "Epoch: 15, Batch: 175, Average Loss: 1.8136\n",
      "Epoch: 15, Batch: 200, Average Loss: 1.8170\n",
      "Epoch 15 - Mean agent Q: 1.5351561307907104, Mean phys Q: 1.4256623983383179\n",
      "New best model saved with mean agent Q: 1.5351561307907104\n",
      "Agent actions: tensor([8, 4, 4, 4, 4, 4, 4, 4, 8, 0, 0, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 16, Batch: 0, Average Loss: 1.9284\n",
      "Epoch: 16, Batch: 25, Average Loss: 1.8543\n",
      "Epoch: 16, Batch: 50, Average Loss: 1.8348\n",
      "Epoch: 16, Batch: 75, Average Loss: 1.8349\n",
      "Epoch: 16, Batch: 100, Average Loss: 1.8365\n",
      "Epoch: 16, Batch: 125, Average Loss: 1.8405\n",
      "Epoch: 16, Batch: 150, Average Loss: 1.8405\n",
      "Epoch: 16, Batch: 175, Average Loss: 1.8432\n",
      "Epoch: 16, Batch: 200, Average Loss: 1.8472\n",
      "Epoch 16 - Mean agent Q: 1.6623090505599976, Mean phys Q: 1.5356110334396362\n",
      "New best model saved with mean agent Q: 1.6623090505599976\n",
      "Agent actions: tensor([2, 4, 4, 4, 4, 4, 4, 4, 8, 8, 0, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 17, Batch: 0, Average Loss: 1.9759\n",
      "Epoch: 17, Batch: 25, Average Loss: 1.8948\n",
      "Epoch: 17, Batch: 50, Average Loss: 1.8758\n",
      "Epoch: 17, Batch: 75, Average Loss: 1.8770\n",
      "Epoch: 17, Batch: 100, Average Loss: 1.8784\n",
      "Epoch: 17, Batch: 125, Average Loss: 1.8829\n",
      "Epoch: 17, Batch: 150, Average Loss: 1.8828\n",
      "Epoch: 17, Batch: 175, Average Loss: 1.8862\n",
      "Epoch: 17, Batch: 200, Average Loss: 1.8911\n",
      "Epoch 17 - Mean agent Q: 1.789236307144165, Mean phys Q: 1.6489622592926025\n",
      "New best model saved with mean agent Q: 1.789236307144165\n",
      "Agent actions: tensor([2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 18, Batch: 0, Average Loss: 2.0401\n",
      "Epoch: 18, Batch: 25, Average Loss: 1.9493\n",
      "Epoch: 18, Batch: 50, Average Loss: 1.9314\n",
      "Epoch: 18, Batch: 75, Average Loss: 1.9330\n",
      "Epoch: 18, Batch: 100, Average Loss: 1.9340\n",
      "Epoch: 18, Batch: 125, Average Loss: 1.9385\n",
      "Epoch: 18, Batch: 150, Average Loss: 1.9381\n",
      "Epoch: 18, Batch: 175, Average Loss: 1.9420\n",
      "Epoch: 18, Batch: 200, Average Loss: 1.9476\n",
      "Epoch 18 - Mean agent Q: 1.9226429462432861, Mean phys Q: 1.7646441459655762\n",
      "New best model saved with mean agent Q: 1.9226429462432861\n",
      "Agent actions: tensor([2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 19, Batch: 0, Average Loss: 2.1166\n",
      "Epoch: 19, Batch: 25, Average Loss: 2.0122\n",
      "Epoch: 19, Batch: 50, Average Loss: 1.9960\n",
      "Epoch: 19, Batch: 75, Average Loss: 1.9969\n",
      "Epoch: 19, Batch: 100, Average Loss: 1.9968\n",
      "Epoch: 19, Batch: 125, Average Loss: 2.0005\n",
      "Epoch: 19, Batch: 150, Average Loss: 1.9993\n",
      "Epoch: 19, Batch: 175, Average Loss: 2.0029\n",
      "Epoch: 19, Batch: 200, Average Loss: 2.0088\n",
      "Epoch 19 - Mean agent Q: 2.034576892852783, Mean phys Q: 1.8739643096923828\n",
      "New best model saved with mean agent Q: 2.034576892852783\n",
      "Agent actions: tensor([2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 20, Batch: 0, Average Loss: 2.1903\n",
      "Epoch: 20, Batch: 25, Average Loss: 2.0704\n",
      "Epoch: 20, Batch: 50, Average Loss: 2.0558\n",
      "Epoch: 20, Batch: 75, Average Loss: 2.0550\n",
      "Epoch: 20, Batch: 100, Average Loss: 2.0535\n",
      "Epoch: 20, Batch: 125, Average Loss: 2.0557\n",
      "Epoch: 20, Batch: 150, Average Loss: 2.0535\n",
      "Epoch: 20, Batch: 175, Average Loss: 2.0561\n",
      "Epoch: 20, Batch: 200, Average Loss: 2.0619\n",
      "Epoch 20 - Mean agent Q: 2.1428773403167725, Mean phys Q: 1.9887900352478027\n",
      "New best model saved with mean agent Q: 2.1428773403167725\n",
      "Agent actions: tensor([2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 21, Batch: 0, Average Loss: 2.2486\n",
      "Epoch: 21, Batch: 25, Average Loss: 2.1145\n",
      "Epoch: 21, Batch: 50, Average Loss: 2.1011\n",
      "Epoch: 21, Batch: 75, Average Loss: 2.0989\n",
      "Epoch: 21, Batch: 100, Average Loss: 2.0960\n",
      "Epoch: 21, Batch: 125, Average Loss: 2.0967\n",
      "Epoch: 21, Batch: 150, Average Loss: 2.0935\n",
      "Epoch: 21, Batch: 175, Average Loss: 2.0952\n",
      "Epoch: 21, Batch: 200, Average Loss: 2.1008\n",
      "Epoch 21 - Mean agent Q: 2.247208833694458, Mean phys Q: 2.103696346282959\n",
      "New best model saved with mean agent Q: 2.247208833694458\n",
      "Agent actions: tensor([2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 22, Batch: 0, Average Loss: 2.2891\n",
      "Epoch: 22, Batch: 25, Average Loss: 2.1441\n",
      "Epoch: 22, Batch: 50, Average Loss: 2.1314\n",
      "Epoch: 22, Batch: 75, Average Loss: 2.1283\n",
      "Epoch: 22, Batch: 100, Average Loss: 2.1242\n",
      "Epoch: 22, Batch: 125, Average Loss: 2.1236\n",
      "Epoch: 22, Batch: 150, Average Loss: 2.1198\n",
      "Epoch: 22, Batch: 175, Average Loss: 2.1207\n",
      "Epoch: 22, Batch: 200, Average Loss: 2.1261\n",
      "Epoch 22 - Mean agent Q: 2.3447816371917725, Mean phys Q: 2.2129650115966797\n",
      "New best model saved with mean agent Q: 2.3447816371917725\n",
      "Agent actions: tensor([2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 23, Batch: 0, Average Loss: 2.3147\n",
      "Epoch: 23, Batch: 25, Average Loss: 2.1607\n",
      "Epoch: 23, Batch: 50, Average Loss: 2.1485\n",
      "Epoch: 23, Batch: 75, Average Loss: 2.1450\n",
      "Epoch: 23, Batch: 100, Average Loss: 2.1400\n",
      "Epoch: 23, Batch: 125, Average Loss: 2.1383\n",
      "Epoch: 23, Batch: 150, Average Loss: 2.1340\n",
      "Epoch: 23, Batch: 175, Average Loss: 2.1342\n",
      "Epoch: 23, Batch: 200, Average Loss: 2.1394\n",
      "Epoch 23 - Mean agent Q: 2.4362094402313232, Mean phys Q: 2.3153955936431885\n",
      "New best model saved with mean agent Q: 2.4362094402313232\n",
      "Agent actions: tensor([2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 24, Batch: 0, Average Loss: 2.3278\n",
      "Epoch: 24, Batch: 25, Average Loss: 2.1664\n",
      "Epoch: 24, Batch: 50, Average Loss: 2.1544\n",
      "Epoch: 24, Batch: 75, Average Loss: 2.1509\n",
      "Epoch: 24, Batch: 100, Average Loss: 2.1453\n",
      "Epoch: 24, Batch: 125, Average Loss: 2.1429\n",
      "Epoch: 24, Batch: 150, Average Loss: 2.1383\n",
      "Epoch: 24, Batch: 175, Average Loss: 2.1379\n",
      "Epoch: 24, Batch: 200, Average Loss: 2.1429\n",
      "Epoch 24 - Mean agent Q: 2.5280323028564453, Mean phys Q: 2.4145915508270264\n",
      "New best model saved with mean agent Q: 2.5280323028564453\n",
      "Agent actions: tensor([2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 25, Batch: 0, Average Loss: 2.3309\n",
      "Epoch: 25, Batch: 25, Average Loss: 2.1641\n",
      "Epoch: 25, Batch: 50, Average Loss: 2.1520\n",
      "Epoch: 25, Batch: 75, Average Loss: 2.1486\n",
      "Epoch: 25, Batch: 100, Average Loss: 2.1428\n",
      "Epoch: 25, Batch: 125, Average Loss: 2.1399\n",
      "Epoch: 25, Batch: 150, Average Loss: 2.1351\n",
      "Epoch: 25, Batch: 175, Average Loss: 2.1342\n",
      "Epoch: 25, Batch: 200, Average Loss: 2.1390\n",
      "Epoch 25 - Mean agent Q: 2.6193244457244873, Mean phys Q: 2.5127646923065186\n",
      "New best model saved with mean agent Q: 2.6193244457244873\n",
      "Agent actions: tensor([2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 26, Batch: 0, Average Loss: 2.3262\n",
      "Epoch: 26, Batch: 25, Average Loss: 2.1554\n",
      "Epoch: 26, Batch: 50, Average Loss: 2.1429\n",
      "Epoch: 26, Batch: 75, Average Loss: 2.1401\n",
      "Epoch: 26, Batch: 100, Average Loss: 2.1343\n",
      "Epoch: 26, Batch: 125, Average Loss: 2.1310\n",
      "Epoch: 26, Batch: 150, Average Loss: 2.1263\n",
      "Epoch: 26, Batch: 175, Average Loss: 2.1249\n",
      "Epoch: 26, Batch: 200, Average Loss: 2.1294\n",
      "Epoch 26 - Mean agent Q: 2.712074041366577, Mean phys Q: 2.609498977661133\n",
      "New best model saved with mean agent Q: 2.712074041366577\n",
      "Agent actions: tensor([2, 2, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "Physician actions: tensor([ 0, 15, 20, 20, 20, 16,  1, 21, 11,  1,  1,  0,  0,  0,  0,  0])\n",
      "Epoch: 27, Batch: 0, Average Loss: 2.3161\n",
      "Epoch: 27, Batch: 25, Average Loss: 2.1419\n",
      "Epoch: 27, Batch: 50, Average Loss: 2.1290\n",
      "Epoch: 27, Batch: 75, Average Loss: 2.1268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 226\u001b[0m\n\u001b[0;32m    223\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[1;32m--> 226\u001b[0m     record \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatchs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     record_loss_z\u001b[38;5;241m.\u001b[39mappend(record)\n\u001b[0;32m    229\u001b[0m     record_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(record_loss_z)\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:96\u001b[0m, in \u001b[0;36mWD3QNE.train\u001b[1;34m(self, batches, epoch)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Create batch tensors\u001b[39;00m\n\u001b[0;32m     88\u001b[0m batch \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     89\u001b[0m     state[batch_mask],\n\u001b[0;32m     90\u001b[0m     next_state[batch_mask],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m     done[batch_mask],\n\u001b[0;32m     94\u001b[0m )\n\u001b[1;32m---> 96\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Log progress\u001b[39;00m\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\proposed_model\\EWD3QN\\WD3QNE_deepQnet.py:140\u001b[0m, in \u001b[0;36mWD3QNE.compute_loss\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# Backpropagation and optimizer step\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 140\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_size\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\icuenv\\lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[1;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\icuenv\\lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\icuenv\\lib\\site-packages\\torch\\optim\\adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    237\u001b[0m         group,\n\u001b[0;32m    238\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         state_steps,\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\icuenv\\lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\icuenv\\lib\\site-packages\\torch\\optim\\adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\TrekAI_ICU\\icuenv\\lib\\site-packages\\torch\\optim\\adam.py:525\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    523\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 525\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.perf_counter()\n",
    "    with open('../../step_4_start_mimiciv_try.pkl', 'rb') as file:\n",
    "        MIMICtable = pickle.load(file)\n",
    "\n",
    "    #####################Pengaturan parameter model##############################\n",
    "    num_epoch = 101  # Number of training cycles\n",
    "    gamma = 0.99\n",
    "    beat1 = 0\n",
    "    beat2 = 0.6\n",
    "    beta3 = 0.3\n",
    "    ncv = 5  # nr of crossvalidation runs (each is 80% training / 20% test)Nr of cross-validation runs (80% training / 20% testing each)\n",
    "    nra = 5\n",
    "    lr = 1e-4\n",
    "    beta = [beat1, beat2, beta3]\n",
    "    icustayidlist = MIMICtable['stay_id']\n",
    "    icuuniqueids = np.unique(icustayidlist)  # list of unique icustayids from MIMIC unique icustayid list\n",
    "    reformat5 = MIMICtable.values.copy()\n",
    "    # Set the seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # # If using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    print('####  generated state  ####')\n",
    "\n",
    "    # -----------------------Filtered features = 37--------------------------------\n",
    "    colnorm = ['SOFA', 'age', 'Weight_kg', 'GCS', 'HR', 'SysBP', 'MeanBP', 'DiaBP', 'RR', 'Temp_C',\n",
    "               'Sodium', 'Chloride', 'Glucose', 'Calcium', 'Hb', 'WBC_count', 'Platelets_count',\n",
    "               'PTT', 'PT', 'Arterial_pH', 'paO2', 'paCO2', 'HCO3', 'Arterial_lactate', 'Shock_Index',\n",
    "               'PaO2_FiO2', 'cumulated_balance', 'CO2_mEqL', 'Ionised_Ca']\n",
    "    ##8个指标\n",
    "    collog = ['SpO2', 'BUN', 'Creatinine', 'SGOT', 'Total_bili', 'INR', 'input_total', 'output_total']\n",
    "\n",
    "    colnorm = np.where(np.isin(MIMICtable.columns, colnorm))[0]\n",
    "    collog = np.where(np.isin(MIMICtable.columns, collog))[0]\n",
    "\n",
    "    scaleMIMIC = np.concatenate([zscore(reformat5[:, colnorm], ddof=1),\n",
    "                                 zscore(np.log(0.1 + reformat5[:, collog]), ddof=1)], axis=1)\n",
    "\n",
    "    # scaleMIMIC = zscore(reformat5[:, colnorm], ddof=1)\n",
    "\n",
    "    train = np.load('../train_mimiciv.npy')\n",
    "    validat = np.load('../validation_mimiciv.npy')\n",
    "    test = np.load('../test_mimiciv.npy')\n",
    "\n",
    "    Xvalidat = scaleMIMIC[validat, :]\n",
    "    blocsvalidat = reformat5[validat, 0]\n",
    "    ptidvalidat = reformat5[validat, 1]\n",
    "\n",
    "    Xtrain = scaleMIMIC[train, :]\n",
    "    Xtest = scaleMIMIC[test, :]\n",
    "    blocstrain = reformat5[train, 0]  # Serial number\n",
    "    bloctest = reformat5[test, 0]\n",
    "    ptidtrain = reformat5[train, 1]  # Patient number\n",
    "    ptidtest = reformat5[test, 1]\n",
    "\n",
    "    # *************************\n",
    "    RNNstate = Xtrain  # ***\n",
    "\n",
    "    print('####  Generating Actions  ####')\n",
    "    nact = nra * nra  # 5*5=25\n",
    "    iol = MIMICtable.columns.get_loc('input_4hourly')  # Columns entered\n",
    "    vcl = MIMICtable.columns.get_loc('max_dose_vaso')  # Columns for maximum use of pressurized drugs\n",
    "\n",
    "    a = reformat5[:, iol].copy()  # IV fluid  Intravenous fluid resuscitation\n",
    "    a = rankdata(a[a > 0]) / a[a > 0].shape[0]  # excludes zero fluid (will be action 1)Excluding zero fluid, it will be action 1.\n",
    "    iof = np.floor((a + 0.2499999999) * 4)  # converts iv volume in 4 actions Switching IV volumes in 4 maneuvers\n",
    "    a = reformat5[:, iol].copy()\n",
    "    a = np.where(a > 0)[0]  # location of non-zero fluid in big matrix\n",
    "    io = np.ones((reformat5.shape[0], 1))  # array of ones, by default\n",
    "    io[a] = (iof + 1).reshape(-1, 1)  # where more than zero fluid given: save actual action\n",
    "    io = io.ravel()  # Both are essentially trying to bring a multi-dimensional array down to 1 dimension\n",
    "    # Injections have 5 actions that have been judged by rank #\n",
    "\n",
    "    vc = reformat5[:, vcl].copy()\n",
    "    vcr = rankdata(vc[vc != 0]) / vc[vc != 0].size\n",
    "    vcr = np.floor((vcr + 0.249999999999) * 4)  # converts to 4 bins\n",
    "    vcr[vcr == 0] = 1\n",
    "    vc[vc != 0] = vcr + 1\n",
    "    vc[vc == 0] = 1\n",
    "\n",
    "    ma1 = np.array(\n",
    "        [np.median(reformat5[io == 1, iol]), np.median(reformat5[io == 2, iol]), np.median(reformat5[io == 3, iol]),\n",
    "         np.median(reformat5[io == 4, iol]), np.median(reformat5[io == 5, iol])])  # median dose of drug in all bins\n",
    "    ma2 = np.array(\n",
    "        [np.median(reformat5[vc == 1, vcl]), np.median(reformat5[vc == 2, vcl]), np.median(reformat5[vc == 3, vcl]),\n",
    "         np.median(reformat5[vc == 4, vcl]), np.median(reformat5[vc == 5, vcl])])\n",
    "\n",
    "    med = np.concatenate([io.reshape(-1, 1), vc.reshape(-1, 1)], axis=1)\n",
    "    uniqueValues, actionbloc = np.unique(med, axis=0, return_inverse=True)\n",
    "\n",
    "    actionbloctrain = actionbloc[train]  # ***\n",
    "    actionblocvalidat = actionbloc[validat]  # ***\n",
    "    actionbloctest = actionbloc[test]\n",
    "\n",
    "    ma2Values = ma2[uniqueValues[:, 1].astype('int64') - 1].reshape(-1, 1)\n",
    "    ma1Values = ma1[uniqueValues[:, 0].astype('int64') - 1].reshape(-1, 1)\n",
    "    uniqueValuesdose = np.concatenate([ma2Values, ma1Values], axis=1)  # median dose of each bin for all 25 actions\n",
    "\n",
    "    # =================incentives============================\n",
    "    print('####  Generate Rewards  ####')\n",
    "    outcome = 9\n",
    "    Y90 = reformat5[train, outcome]\n",
    "    reward_value = 24  \n",
    "    r = np.array([reward_value, -reward_value]).reshape(1, -1)\n",
    "    r2 = r * (2 * (1 - Y90.reshape(-1, 1)) - 1) # Skor reward\n",
    "\n",
    "    # -----Incentive function preparation-----------------------------\n",
    "    SOFA = reformat5[train, 57]  # ***\n",
    "    R3 = r2[:, 0]\n",
    "    R4 = (R3 + reward_value) / (2 * reward_value)\n",
    "    c = 0\n",
    "    bloc_max = max(blocstrain)\n",
    "\n",
    "    # ================Build state & & next state sequence lists Generate strategy trajectories=================================\n",
    "    print(RNNstate.shape[0])\n",
    "\n",
    "    print('####  Generate trajectory  ####')\n",
    "    statesize = int(RNNstate.shape[1])\n",
    "    states = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), statesize))\n",
    "    actions = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), 1), dtype=int)\n",
    "    next_actions = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), 1), dtype=int)\n",
    "    rewards = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), 1))\n",
    "    next_states = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), statesize))\n",
    "    done_flags = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), 1))\n",
    "    bloc_num = np.zeros((np.floor(RNNstate.shape[0] * 1.2).astype('int64'), 1))\n",
    "    blocnum1 = 1\n",
    "\n",
    "    bloc_num_reward = 0\n",
    "    for i in range(RNNstate.shape[0] - 1):  # Each line of the cycle\n",
    "        states[c] = RNNstate[i, :]\n",
    "        actions[c] = actionbloctrain[i]\n",
    "        bloc_num[c] = blocnum1\n",
    "        if (blocstrain[i + 1] == 1):  # end of trace for this patient\n",
    "            next_states1 = np.zeros(statesize)\n",
    "            next_actions1 = -1\n",
    "            done_flags1 = 1\n",
    "            blocnum1 = blocnum1 + 1\n",
    "            bloc_num_reward += 1\n",
    "            reward1 = -beat1 * (SOFA[i]) + R3[i]\n",
    "            bloc_num_reward = 0\n",
    "        else:\n",
    "            next_states1 = RNNstate[i + 1, :]\n",
    "            next_actions1 = actionbloctrain[i + 1]\n",
    "            done_flags1 = 0\n",
    "            blocnum1 = blocnum1\n",
    "            reward1 = - beat2 * (SOFA[i + 1] - SOFA[i])\n",
    "            bloc_num_reward += 1\n",
    "        next_states[c] = next_states1\n",
    "        next_actions[c] = next_actions1\n",
    "        rewards[c] = reward1\n",
    "        done_flags[c] = done_flags1\n",
    "        c = c + 1\n",
    "    states[c] = RNNstate[c, :]\n",
    "    actions[c] = actionbloctrain[c]\n",
    "    bloc_num[c] = blocnum1\n",
    "\n",
    "    next_states1 = np.zeros(statesize)\n",
    "    next_actions1 = -1\n",
    "    done_flags1 = 1\n",
    "    blocnum1 = blocnum1 + 1\n",
    "    bloc_num_reward += 1\n",
    "    reward1 = -beat1 * (SOFA[c]) + R3[c]\n",
    "\n",
    "    bloc_num_reward = 0\n",
    "    next_states[c] = next_states1\n",
    "    next_actions[c] = next_actions1\n",
    "    rewards[c] = reward1\n",
    "    done_flags[c] = done_flags1\n",
    "    c = c + 1\n",
    "\n",
    "    bloc_num[c] = blocnum1\n",
    "    bloc_num = bloc_num[:c, :]\n",
    "    states = states[: c, :]\n",
    "    next_states = next_states[: c, :]\n",
    "    actions = actions[: c, :]\n",
    "    next_actions = next_actions[: c, :]\n",
    "    rewards = rewards[: c, :]\n",
    "    done_flags = done_flags[: c, :]\n",
    "    bloc_num = np.squeeze(bloc_num)\n",
    "    actions = np.squeeze(actions)\n",
    "    rewards = np.squeeze(rewards)\n",
    "    done_flags = np.squeeze(done_flags)\n",
    "    batch_size = 16\n",
    "    state = torch.FloatTensor(states).to(device)\n",
    "    next_state = torch.FloatTensor(next_states).to(device)\n",
    "    action = torch.LongTensor(actions).to(device)\n",
    "    next_action = torch.LongTensor(next_actions).to(device)\n",
    "    reward = torch.FloatTensor(rewards).to(device)\n",
    "    done = torch.FloatTensor(done_flags).to(device)\n",
    "    SOFAS = torch.LongTensor(SOFA).to(device)\n",
    "    batchs = (state, next_state, action, next_action, reward, done, bloc_num, SOFAS)\n",
    "\n",
    "# =================Training model, master cycle==================\n",
    "Y90_validat = reformat5[validat, outcome]\n",
    "SOFA_validat = reformat5[validat, 57]\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = WD3QNE(state_dim=37, num_actions=25)\n",
    "\n",
    "# Instantiating Network Models\n",
    "record_loss_z = []\n",
    "record_phys_q = []\n",
    "record_agent_q = []\n",
    "# Inisialisasi dengan nilai yang sangat rendah\n",
    "best_mean_agent_q = -float('inf')\n",
    "save_dir = 'EWD3QN-algorithm/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    record = model.train(batchs, epoch)\n",
    "    record_loss_z.append(record)\n",
    "\n",
    "    record_a = np.array(record_loss_z)\n",
    "    record_b = np.sum(record_a, axis=1)\n",
    "\n",
    "    # -------------Validation Sets, Evaluation------------------------------\n",
    "    uids = np.unique(bloc_num)\n",
    "    batch_s = ptidvalidat  # Example: unique patient IDs\n",
    "    batch_user = np.isin(bloc_num, (1, len(batch_s)))\n",
    "    state_user = state[batch_user, :]\n",
    "    action_user = action[batch_user]\n",
    "    next_state_user = next_state[batch_user, :]\n",
    "    next_action_user = next_action[batch_user]\n",
    "    reward_user = reward[batch_user]\n",
    "    done_user = done[batch_user]\n",
    "    batch = (state_user, next_state_user, action_user,\n",
    "             next_action_user, reward_user, done_user)\n",
    "\n",
    "    q_output, agent_actions, phys_actions, Q_value_pro = do_eval(model, batch)\n",
    "\n",
    "    agent_q = q_output[:, agent_actions]\n",
    "    phys_q = q_output[:, phys_actions]\n",
    "\n",
    "    mean_agent_q = torch.mean(agent_q).item()\n",
    "    mean_phys_q = torch.mean(phys_q).item()\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch} - Mean agent Q: {mean_agent_q}, Mean phys Q: {mean_phys_q}')\n",
    "\n",
    "    record_phys_q.append(mean_phys_q)\n",
    "    record_agent_q.append(mean_agent_q)\n",
    "\n",
    "    # Save the model if it has the best performance so far\n",
    "    if mean_agent_q > best_mean_agent_q:\n",
    "        best_mean_agent_q = mean_agent_q\n",
    "        best_model_save_path = os.path.join(save_dir, 'best_model.pt')\n",
    "\n",
    "        model_save_dict = {\n",
    "            'Q_state_dicts': [q_net.state_dict() for q_net in model.Q_ensemble],\n",
    "            'Q_target_state_dicts': [q_net.state_dict() for q_net in model.Q_target_ensemble],\n",
    "            'best_mean_agent_q': best_mean_agent_q\n",
    "        }\n",
    "        torch.save(model_save_dict, best_model_save_path)\n",
    "        print(f'New best model saved with mean agent Q: {best_mean_agent_q}')\n",
    "\n",
    "    print('Agent actions:', agent_actions)\n",
    "    print('Physician actions:', phys_actions)\n",
    "\n",
    "\n",
    "# ===========picture (e.g. of life in the city)=============================\n",
    "x_length_list = list(range(len(record_b)))\n",
    "plt.figure()\n",
    "plt.title('Training')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(x_length_list, record_b)\n",
    "np.save('EWD3QN-algorithm/loss.npy', record_b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####  Generating test set traces  ####\n",
      "Time used: 918.7198878999916\n"
     ]
    }
   ],
   "source": [
    "# agent_length_list = list(range(len(record_agent_q)))\n",
    "#     plt.figure()\n",
    "#     plt.title('Training')\n",
    "#     plt.xlabel(\"epoch\")\n",
    "#     plt.ylabel(\"mean Q value\")\n",
    "\n",
    "#     # Plot the data\n",
    "#     record_agent_q_np = [q.detach().numpy() if isinstance(q, torch.Tensor) else q for q in record_agent_q]\n",
    "#     plt.plot(agent_length_list, record_agent_q_np, label='record_agent_q')\n",
    "#     record_agent_q_np = np.array(record_agent_q_np)  # Convert the list to a NumPy array\n",
    "#     np.save('ID3QNE-algorithm/mean_agent_q.npy', record_agent_q_np)\n",
    "\n",
    "#     record_phys_q_np = [q.detach().numpy() if isinstance(q, torch.Tensor) else q for q in record_phys_q]\n",
    "#     record_phys_q_np = np.array(record_phys_q_np)  # Ensure it's a NumPy array\n",
    "\n",
    "#     phys_length_list = list(range(len(record_phys_q_np)))\n",
    "#     np.save('ID3QNE-algorithm/mean_phys_q.npy', record_phys_q_np)\n",
    "\n",
    "# Convert and save record_agent_q\n",
    "record_agent_q_np = [\n",
    "    q.detach().cpu().numpy() if isinstance(q, torch.Tensor) else q \n",
    "    for q in record_agent_q\n",
    "]\n",
    "# Convert the list to a NumPy array\n",
    "record_agent_q_np = np.array(record_agent_q_np)\n",
    "agent_length_list = list(range(len(record_agent_q_np)))\n",
    "plt.plot(agent_length_list, record_agent_q_np, label='record_agent_q')\n",
    "np.save('EWD3QN-algorithm/mean_agent_q.npy', record_agent_q_np)\n",
    "\n",
    "# Convert and save record_phys_q\n",
    "record_phys_q_np = [\n",
    "    q.detach().cpu().numpy() if isinstance(q, torch.Tensor) else q \n",
    "    for q in record_phys_q\n",
    "]\n",
    "record_phys_q_np = np.array(record_phys_q_np)  # Ensure it's a NumPy array\n",
    "phys_length_list = list(range(len(record_phys_q_np)))\n",
    "np.save('EWD3QN-algorithm/mean_phys_q.npy', record_phys_q_np)\n",
    "\n",
    "# Plot configurations (if needed)\n",
    "plt.ylabel(\"mean Q value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =================测试集，评估test set================================================================\n",
    "Y90_test = reformat5[test, outcome]\n",
    "SOFA_test = reformat5[test, 57]\n",
    "do_test(model, Xtest, actionbloctest, bloctest, Y90_test, SOFA_test, reward_value, beta)\n",
    "\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print(\"Time used:\", elapsed)\n",
    "plt.show()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
